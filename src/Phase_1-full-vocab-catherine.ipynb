{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable difference is that I cleaned up the BERT vocab to strip non-ascii characters, reserved tokens, etc. Recreating some of the layers (i.e. visual) was going to take an estimated 120 hours so I just loaded the other saved layers from Dr. Kennington and intersected vocab in later steps (`make_big_network_catherine.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 19:25:35.439207: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from flair.embeddings import WordEmbeddings, TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "import networkx as nx\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import torch\n",
    "from networkx.algorithms import approximation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_thresh = 0.9\n",
    "visual_thresh = 0.9\n",
    "lancaster_thresh = 0.98\n",
    "free_asoc_threshold =  0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get BERT vocab (all other layers will be limited to words found in BERT vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embedding = WordEmbeddings('glove')\n",
    "embedding = TransformerWordEmbeddings('bert-base-uncased', layers='-1', layer_mean=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# embd_vocab  = list(glove_embedding.precomputed_word_embeddings.key_to_index.keys())\n",
    "\n",
    "# len(embd_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vocab = list(tokenizer.vocab.keys())\n",
    "\n",
    "len(bert_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_dict = dict(enumerate(bert_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the ranges where it's clearly not words (just symbols and tokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, 1037):\n",
    "    bert_vocab_dict.pop(i, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1038, 1996):\n",
    "    bert_vocab_dict.pop(i, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(29613, len(bert_vocab_dict.keys())):\n",
    "    bert_vocab_dict.pop(i, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28527"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip words that begin w/ '#' sign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_dict = {key: val.strip(\"##\") for key, val in bert_vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28527"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words that are just numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_dict = {key: val for key, val in bert_vocab_dict.items() if not val.isnumeric()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27567"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words with a length of 1\n",
    "\n",
    "except for \"a\" and \"i\" since they're valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_dict = {key: val for key, val in bert_vocab_dict.items() if len(val) > 1 or val in (\"a\", \"i\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26615"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words with non-ascii characters\n",
    "\n",
    "Should catch characters/words like 'ы', 'ν' 'я', etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab_dict = {key: val for key, val in bert_vocab_dict.items() if val.isascii()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26573"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bert_vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes_w_odd_characters = [2134, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab = list(bert_vocab_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26573"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(bert_vocab, open( \"cleaned_bert_vocab.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the Multiplex Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Shared functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_degree_connectivity(graph_name):\n",
    "    num_edges = 0\n",
    "    for node in graph_name.nodes():\n",
    "        num_edges += len(graph_name.edges(node))\n",
    "\n",
    "    return num_edges/graph_name.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_nodes_in_lcc(graph_name):\n",
    "    largest_cc = max(nx.connected_components(graph_name), key=len)\n",
    "    lenght = len(largest_cc)\n",
    "    return lenght/graph_name.number_of_nodes()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shortest_path_lcc(graph_name):\n",
    "    S = [graph_name.subgraph(c).copy() for c in nx.connected_components(graph_name)]\n",
    "    comps = [len(max(nx.connected_components(i), key=len)) for i in S]\n",
    "    index_max = max(range(len(comps)), key=comps.__getitem__)\n",
    "    return nx.average_shortest_path_length(S[index_max])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create free association layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "faw = [line.strip().split('\\t') for line in open('../data/fullmultiplex/freeassoc_full.csv')]\n",
    "words = list(set(line[0] for line in faw))\n",
    "# faw = [line for line in faw if len(line) == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'one', '21', '97', '0.216494845360825'],\n",
       " ['a', 'the', '16', '97', '0.164948453608247'],\n",
       " ['a', 'b', '9', '97', '0.0927835051546392'],\n",
       " ['a', 'an', '4', '97', '0.0412371134020619'],\n",
       " ['a', 'first', '3', '97', '0.0309278350515464'],\n",
       " ['a', 'letter', '3', '97', '0.0309278350515464'],\n",
       " ['a', 'alphabet', '2', '97', '0.0206185567010309'],\n",
       " ['a', 'apple', '2', '97', '0.0206185567010309'],\n",
       " ['a', 'article', '2', '97', '0.0206185567010309'],\n",
       " ['a', 'bat', '2', '97', '0.0206185567010309']]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faw[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "faw_intersect_bert = [word for word in faw if (faw[0] in bert_vocab and faw[1] in bert_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "faw_intersect_bert = words_intersect_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original word count: 483636 vs new word count: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"original word count: {len(faw)} vs new word count: {len(faw_intersect_bert)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a\\tone', 'a\\tthe', 'aardvark\\tanimal', 'aardvark\\tanteater', 'abacus\\tmath']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_association_words =  ['\\t'.join(line[:2]) for line in faw_intersect_bert if float(line[4]) > free_asoc_threshold]\n",
    "\n",
    "free_association_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data and store it into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_association_list = []\n",
    "\n",
    "for pair in free_association_words:\n",
    "    free_association_list.append(tuple(map(str, pair.split('\\t'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'one'),\n",
       " ('a', 'the'),\n",
       " ('aardvark', 'animal'),\n",
       " ('aardvark', 'anteater'),\n",
       " ('abacus', 'math'),\n",
       " ('abacus', 'counting'),\n",
       " ('abandon', 'leave'),\n",
       " ('abbey', 'church'),\n",
       " ('abbey', 'road'),\n",
       " ('abbey', 'monk'),\n",
       " ('abbreviation', 'short'),\n",
       " ('abbreviation', 'shorten'),\n",
       " ('abdicate', 'give up'),\n",
       " ('abdomen', 'stomach'),\n",
       " ('abdomen', 'belly'),\n",
       " ('abdomen', 'tummy'),\n",
       " ('abduct', 'kidnap'),\n",
       " ('abduct', 'steal'),\n",
       " ('abduct', 'take'),\n",
       " ('abduction', 'kidnap'),\n",
       " ('abduction', 'kidnapping'),\n",
       " ('Abel', 'Cain'),\n",
       " ('abhor', 'hate'),\n",
       " ('abide', 'stay'),\n",
       " ('ability', 'skill'),\n",
       " ('ability', 'talent'),\n",
       " ('able', 'capable'),\n",
       " ('able', 'can'),\n",
       " ('able', 'willing'),\n",
       " ('able', 'bodied'),\n",
       " ('abnormal', 'weird'),\n",
       " ('abnormal', 'strange'),\n",
       " ('abode', 'home'),\n",
       " ('abode', 'house'),\n",
       " ('abode', 'humble'),\n",
       " ('Aboriginal', 'Australia'),\n",
       " ('Aboriginal', 'native'),\n",
       " ('aborigine', 'Australia'),\n",
       " ('aborigine', 'native'),\n",
       " ('abortion', 'baby'),\n",
       " ('about', 'around'),\n",
       " ('above', 'below'),\n",
       " ('above', 'over'),\n",
       " ('above', 'beyond'),\n",
       " ('above', 'up'),\n",
       " ('abrasive', 'rough'),\n",
       " ('abrasive', 'sandpaper'),\n",
       " ('abroad', 'overseas'),\n",
       " ('abroad', 'travel'),\n",
       " ('abrupt', 'sudden'),\n",
       " ('abrupt', 'stop'),\n",
       " ('abrupt', 'quick'),\n",
       " ('abs', 'muscles'),\n",
       " ('abs', 'stomach'),\n",
       " ('absence', 'gone'),\n",
       " ('absent', 'gone'),\n",
       " ('absent', 'missing'),\n",
       " ('absent', 'away'),\n",
       " ('absolute', 'total'),\n",
       " ('absolute', 'vodka'),\n",
       " ('absolutely', 'positively'),\n",
       " ('absolutely', 'definitely'),\n",
       " ('absorb', 'sponge'),\n",
       " ('absorbent', 'towel'),\n",
       " ('absorbent', 'sponge'),\n",
       " ('abstain', 'sex'),\n",
       " ('abstain', 'refrain'),\n",
       " ('abstract', 'art'),\n",
       " ('abstract', 'painting'),\n",
       " ('absurd', 'silly'),\n",
       " ('absurd', 'ridiculous'),\n",
       " ('absurd', 'crazy'),\n",
       " ('abundance', 'plenty'),\n",
       " ('abundant', 'plentiful'),\n",
       " ('abundant', 'plenty'),\n",
       " ('abundant', 'lots'),\n",
       " ('abuse', 'hurt'),\n",
       " ('abuse', 'child'),\n",
       " ('abusive', 'relationship'),\n",
       " ('academia', 'school'),\n",
       " ('academia', 'university'),\n",
       " ('academia', 'study'),\n",
       " ('academic', 'school'),\n",
       " ('academy', 'school'),\n",
       " ('accelerate', 'speed'),\n",
       " ('accelerate', 'car'),\n",
       " ('accelerate', 'fast'),\n",
       " ('acceleration', 'speed'),\n",
       " ('acceleration', 'fast'),\n",
       " ('acceleration', 'car'),\n",
       " ('accent', 'foreign'),\n",
       " ('accent', 'language'),\n",
       " ('accept', 'agree'),\n",
       " ('acceptable', 'okay'),\n",
       " ('acceptable', 'good'),\n",
       " ('acceptance', 'love'),\n",
       " ('access', 'denied'),\n",
       " ('access', 'enter'),\n",
       " ('accessory', 'jewelry'),\n",
       " ('accessory', 'purse'),\n",
       " ('accident', 'car'),\n",
       " ('accident', 'crash'),\n",
       " ('accompany', 'with'),\n",
       " ('accomplish', 'achieve'),\n",
       " ('accomplish', 'succeed'),\n",
       " ('accomplishment', 'success'),\n",
       " ('accomplishment', 'achievement'),\n",
       " ('accord', 'agreement'),\n",
       " ('accord', 'Honda'),\n",
       " ('accord', 'agree'),\n",
       " ('accordion', 'music'),\n",
       " ('accordion', 'instrument'),\n",
       " ('account', 'bank'),\n",
       " ('account', 'money'),\n",
       " ('accountant', 'money'),\n",
       " ('accountant', 'numbers'),\n",
       " ('accounting', 'money'),\n",
       " ('accounting', 'numbers'),\n",
       " ('accumulate', 'gather'),\n",
       " ('accuracy', 'correct'),\n",
       " ('accuracy', 'precision'),\n",
       " ('accurate', 'correct'),\n",
       " ('accurate', 'precise'),\n",
       " ('accurate', 'right'),\n",
       " ('accusation', 'guilty'),\n",
       " ('accuse', 'blame'),\n",
       " ('ace', 'card'),\n",
       " ('ace', 'cards'),\n",
       " ('ache', 'pain'),\n",
       " ('ache', 'head'),\n",
       " ('achievement', 'goal'),\n",
       " ('achievement', 'success'),\n",
       " ('achiever', 'success'),\n",
       " ('achiever', 'high'),\n",
       " ('achiever', 'winner'),\n",
       " ('aching', 'pain'),\n",
       " ('aching', 'sore'),\n",
       " ('aching', 'back'),\n",
       " ('acid', 'base'),\n",
       " ('acid', 'rain'),\n",
       " ('acknowledge', 'know'),\n",
       " ('acne', 'spots'),\n",
       " ('acne', 'pimple'),\n",
       " ('acne', 'face'),\n",
       " ('acorn', 'tree'),\n",
       " ('acorn', 'nut'),\n",
       " ('acorn', 'oak'),\n",
       " ('acorn', 'squirrel'),\n",
       " ('acoustic', 'guitar'),\n",
       " ('acoustic', 'sound'),\n",
       " ('acquaintance', 'friend'),\n",
       " ('acquire', 'get'),\n",
       " ('acquire', 'gain'),\n",
       " ('acre', 'land'),\n",
       " ('acre', 'field'),\n",
       " ('acrid', 'smell'),\n",
       " ('acrid', 'bitter'),\n",
       " ('acrid', 'dry'),\n",
       " ('acrid', 'smoke'),\n",
       " ('acrobat', 'circus'),\n",
       " ('acrobatics', 'gymnastics'),\n",
       " ('across', 'over'),\n",
       " ('act', 'play'),\n",
       " ('acting', 'theater'),\n",
       " ('action', 'reaction'),\n",
       " ('actions', 'words'),\n",
       " ('activate', 'start'),\n",
       " ('actor', 'actress'),\n",
       " ('actress', 'actor'),\n",
       " ('actual', 'real'),\n",
       " ('actually', 'really'),\n",
       " ('acute', 'sharp'),\n",
       " ('acute', 'angle'),\n",
       " ('acute', 'pain'),\n",
       " ('acute', 'small'),\n",
       " ('ad', 'commercial'),\n",
       " ('Adam', 'eve'),\n",
       " ('adapt', 'change'),\n",
       " ('add', 'subtract'),\n",
       " ('add', 'plus'),\n",
       " ('added', 'subtracted'),\n",
       " ('added', 'plus'),\n",
       " ('addict', 'drugs'),\n",
       " ('addict', 'drug'),\n",
       " ('addicted', 'drugs'),\n",
       " ('addiction', 'drugs'),\n",
       " ('addiction', 'drug'),\n",
       " ('addictive', 'drugs'),\n",
       " ('addition', 'subtraction'),\n",
       " ('addition', 'plus'),\n",
       " ('addition', 'math'),\n",
       " ('additional', 'more'),\n",
       " ('additional', 'extra'),\n",
       " ('additional', 'plus'),\n",
       " ('additionally', 'also'),\n",
       " ('additive', 'sugar'),\n",
       " ('address', 'home'),\n",
       " ('adept', 'skilled'),\n",
       " ('adept', 'able'),\n",
       " ('adequate', 'enough'),\n",
       " ('adhere', 'stick'),\n",
       " ('adhesive', 'glue'),\n",
       " ('adhesive', 'sticky'),\n",
       " ('adhesive', 'tape'),\n",
       " ('adieu', 'goodbye'),\n",
       " ('adjacent', 'next'),\n",
       " ('adjacent', 'next to'),\n",
       " ('adjacent', 'near'),\n",
       " ('adjective', 'noun'),\n",
       " ('adjourn', 'end'),\n",
       " ('adjust', 'change'),\n",
       " ('adjustment', 'change'),\n",
       " ('administrator', 'boss'),\n",
       " ('admirable', 'good'),\n",
       " ('admiral', 'navy'),\n",
       " ('admiration', 'love'),\n",
       " ('admiration', 'respect'),\n",
       " ('admire', 'like'),\n",
       " ('admire', 'love'),\n",
       " ('admission', 'college'),\n",
       " ('admission', 'ticket'),\n",
       " ('admit', 'confess'),\n",
       " ('adobe', 'house'),\n",
       " ('adobe', 'Photoshop'),\n",
       " ('adolescence', 'youth'),\n",
       " ('adolescence', 'teen'),\n",
       " ('adolescence', 'teenager'),\n",
       " ('adolescent', 'teen'),\n",
       " ('adolescent', 'teenager'),\n",
       " ('adolescent', 'young'),\n",
       " ('adolescent', 'youth'),\n",
       " ('adopt', 'child'),\n",
       " ('adoption', 'child'),\n",
       " ('adoption', 'children'),\n",
       " ('adoption', 'baby'),\n",
       " ('adorable', 'cute'),\n",
       " ('adoration', 'love'),\n",
       " ('adore', 'love'),\n",
       " ('adored', 'loved'),\n",
       " ('adorn', 'decorate'),\n",
       " ('adorn', 'wear'),\n",
       " ('adornment', 'decoration'),\n",
       " ('adornment', 'jewelry'),\n",
       " ('adrenal', 'gland'),\n",
       " ('adrenaline', 'rush'),\n",
       " ('adult', 'child'),\n",
       " ('adultery', 'cheating'),\n",
       " ('adultery', 'sex'),\n",
       " ('adultery', 'cheat'),\n",
       " ('adults', 'children'),\n",
       " ('advance', 'forward'),\n",
       " ('advancement', 'progress'),\n",
       " ('adventure', 'fun'),\n",
       " ('adventurer', 'explorer'),\n",
       " ('adverb', 'adjective'),\n",
       " ('adverb', 'verb'),\n",
       " ('adverb', 'noun'),\n",
       " ('adversary', 'enemy'),\n",
       " ('adversary', 'foe'),\n",
       " ('adversary', 'opponent'),\n",
       " ('adversity', 'difficulty'),\n",
       " ('advert', 'advertisement'),\n",
       " ('advert', 'commercial'),\n",
       " ('advertise', 'sell'),\n",
       " ('advertising', 'marketing'),\n",
       " ('advice', 'help'),\n",
       " ('Advil', 'pain'),\n",
       " ('Advil', 'medicine'),\n",
       " ('Advil', 'ibuprofen'),\n",
       " ('advise', 'help'),\n",
       " ('advise', 'counsel'),\n",
       " ('advisor', 'teacher'),\n",
       " ('advocate', 'lawyer'),\n",
       " ('aerobics', 'exercise'),\n",
       " ('aerosol', 'spray'),\n",
       " ('aerosol', 'can'),\n",
       " ('aesthetics', 'beauty'),\n",
       " ('aesthetics', 'art'),\n",
       " ('afar', 'distant'),\n",
       " ('afar', 'away'),\n",
       " ('afar', 'distance'),\n",
       " ('a few', 'some'),\n",
       " ('a few', 'many'),\n",
       " ('affair', 'love'),\n",
       " ('affairs', 'business'),\n",
       " ('affect', 'effect'),\n",
       " ('affect', 'cause'),\n",
       " ('affection', 'love'),\n",
       " ('affectionate', 'loving'),\n",
       " ('affectionate', 'love'),\n",
       " ('affirm', 'yes'),\n",
       " ('affirm', 'confirm'),\n",
       " ('affirmative', 'yes'),\n",
       " ('affirmative', 'action'),\n",
       " ('affluent', 'rich'),\n",
       " ('affluent', 'wealthy'),\n",
       " ('afford', 'cost'),\n",
       " ('afford', 'money'),\n",
       " ('affordable', 'cheap'),\n",
       " ('afraid', 'scared'),\n",
       " ('afraid', 'fear'),\n",
       " ('Africa', 'continent'),\n",
       " ('Africa', 'black'),\n",
       " ('African', 'black'),\n",
       " ('African', 'American'),\n",
       " ('after', 'before'),\n",
       " ('after', 'later'),\n",
       " ('afternoon', 'morning'),\n",
       " ('afternoon', 'tea'),\n",
       " ('aftershave', 'smell'),\n",
       " ('aftershave', 'cologne'),\n",
       " ('afterwards', 'before'),\n",
       " ('afterwards', 'later'),\n",
       " ('again', 'repeat'),\n",
       " ('again', 'over'),\n",
       " ('against', 'for'),\n",
       " ('against', 'anti'),\n",
       " ('age', 'old'),\n",
       " ('aged', 'old'),\n",
       " ('ageless', 'timeless'),\n",
       " ('ageless', 'young'),\n",
       " ('agenda', 'plan'),\n",
       " ('agent', 'secret'),\n",
       " ('agent', 'representative'),\n",
       " ('ages', 'old'),\n",
       " ('ages', 'eons'),\n",
       " ('aggravate', 'annoy'),\n",
       " ('aggravate', 'anger'),\n",
       " ('aggression', 'anger'),\n",
       " ('aggressive', 'angry'),\n",
       " ('aghast', 'shocked'),\n",
       " ('aghast', 'horrified'),\n",
       " ('agile', 'flexible'),\n",
       " ('agility', 'fast'),\n",
       " ('agility', 'speed'),\n",
       " ('aging', 'old'),\n",
       " ('agitate', 'annoy'),\n",
       " ('agitated', 'angry'),\n",
       " ('agitated', 'upset'),\n",
       " ('agnostic', 'God'),\n",
       " ('agnostic', 'religion'),\n",
       " ('ago', 'long'),\n",
       " ('ago', 'past'),\n",
       " ('agony', 'pain'),\n",
       " ('agree', 'disagree'),\n",
       " ('agree', 'yes'),\n",
       " ('agree', 'concur'),\n",
       " ('agreeable', 'nice'),\n",
       " ('agreed', 'yes'),\n",
       " ('agreement', 'contract'),\n",
       " ('agreement', 'yes'),\n",
       " ('agriculture', 'farming'),\n",
       " ('agriculture', 'farm'),\n",
       " ('ahead', 'behind'),\n",
       " ('ahead', 'forward'),\n",
       " ('ahead', 'go'),\n",
       " ('ahoy', 'matey'),\n",
       " ('ahoy', 'pirate'),\n",
       " ('aid', 'help'),\n",
       " ('aid', 'first'),\n",
       " ('aide', 'help'),\n",
       " ('aide', 'helper'),\n",
       " ('aids', 'disease'),\n",
       " ('aids', 'help'),\n",
       " ('aids', 'HIV'),\n",
       " ('ailment', 'illness'),\n",
       " ('ailment', 'sickness'),\n",
       " ('ailment', 'sick'),\n",
       " ('aim', 'shoot'),\n",
       " ('aim', 'goal'),\n",
       " ('aim', 'arrow'),\n",
       " ('aimless', 'wandering'),\n",
       " ('aimless', 'wander'),\n",
       " ('air', 'breathe'),\n",
       " ('air conditioning', 'cool'),\n",
       " ('air conditioning', 'cold'),\n",
       " ('aircraft', 'plane'),\n",
       " ('aircraft', 'carrier'),\n",
       " ('air force', 'military'),\n",
       " ('air force', 'army'),\n",
       " ('airhead', 'blonde'),\n",
       " ('airhead', 'dumb'),\n",
       " ('airless', 'vacuum'),\n",
       " ('airline', 'plane'),\n",
       " ('airplane', 'fly'),\n",
       " ('airport', 'plane'),\n",
       " ('airport', 'planes'),\n",
       " ('airship', 'plane'),\n",
       " ('airship', 'Zeppelin'),\n",
       " ('airy', 'light'),\n",
       " ('airy', 'fairy'),\n",
       " ('aisle', 'wedding'),\n",
       " ('aisle', 'church'),\n",
       " ('alarm', 'clock'),\n",
       " ('alarm', 'fire'),\n",
       " ('alarm', 'bell'),\n",
       " ('Alaska', 'cold'),\n",
       " ('Alaska', 'state'),\n",
       " ('Albert', 'Einstein'),\n",
       " ('Albert', 'prince'),\n",
       " ('album', 'music'),\n",
       " ('album', 'photo'),\n",
       " ('album', 'record'),\n",
       " ('alchemy', 'gold'),\n",
       " ('alchemy', 'chemistry'),\n",
       " ('alchemy', 'magic'),\n",
       " ('alcohol', 'booze'),\n",
       " ('alcohol', 'drink'),\n",
       " ('alcohol', 'drunk'),\n",
       " ('alcoholic', 'drunk'),\n",
       " ('alcoholic', 'drink'),\n",
       " ('alcoholism', 'drink'),\n",
       " ('alcoholism', 'drunk'),\n",
       " ('ale', 'beer'),\n",
       " ('alert', 'awake'),\n",
       " ('alert', 'alarm'),\n",
       " ('alert', 'red'),\n",
       " ('algae', 'green'),\n",
       " ('algebra', 'math'),\n",
       " ('algebra', 'maths'),\n",
       " ('algorithm', 'math'),\n",
       " ('alibi', 'excuse'),\n",
       " ('alibi', 'crime'),\n",
       " ('alien', 'foreign'),\n",
       " ('alien', 'space'),\n",
       " ('aliens', 'space'),\n",
       " ('aliens', 'green'),\n",
       " ('align', 'straighten'),\n",
       " ('align', 'straight'),\n",
       " ('alike', 'similar'),\n",
       " ('alike', 'same'),\n",
       " ('a little', 'a lot'),\n",
       " ('a little', 'bit'),\n",
       " ('a little', 'small'),\n",
       " ('a little', 'a bit'),\n",
       " ('alive', 'dead'),\n",
       " ('alive', 'living'),\n",
       " ('all', 'everyone'),\n",
       " ('all', 'every'),\n",
       " ('all', 'everything'),\n",
       " ('allegiance', 'loyalty'),\n",
       " ('allegiance', 'pledge'),\n",
       " ('allegiance', 'flag'),\n",
       " ('allergic', 'reaction'),\n",
       " ('allergies', 'sneeze'),\n",
       " ('allergies', 'pollen'),\n",
       " ('allergy', 'sneeze'),\n",
       " ('alley', 'cat'),\n",
       " ('alley', 'dark'),\n",
       " ('alley', 'way'),\n",
       " ('alley', 'lane'),\n",
       " ('alligator', 'crocodile'),\n",
       " ('allow', 'let'),\n",
       " ('allow', 'permit'),\n",
       " ('allowance', 'money'),\n",
       " ('allowed', 'permitted'),\n",
       " ('allowed', 'let'),\n",
       " ('allowed', 'permission'),\n",
       " ('alloy', 'metal'),\n",
       " ('allspice', 'spice'),\n",
       " ('ally', 'friend'),\n",
       " ('almanac', 'calendar'),\n",
       " ('almanac', 'book'),\n",
       " ('almighty', 'God'),\n",
       " ('almond', 'nut'),\n",
       " ('almonds', 'nuts'),\n",
       " ('almost', 'nearly'),\n",
       " ('almost', 'there'),\n",
       " ('aloe', 'Vera'),\n",
       " ('aloe', 'plant'),\n",
       " ('alone', 'lonely'),\n",
       " ('along', 'with'),\n",
       " ('along', 'come'),\n",
       " ('alongside', 'beside'),\n",
       " ('alongside', 'next to'),\n",
       " ('alongside', 'with'),\n",
       " ('alot', 'many'),\n",
       " ('alot', 'much'),\n",
       " ('a lot', 'many'),\n",
       " ('a lot', 'much'),\n",
       " ('alpha', 'beta'),\n",
       " ('alpha', 'male'),\n",
       " ('alpha', 'Omega'),\n",
       " ('alphabet', 'letters'),\n",
       " ('alphabet', 'soup'),\n",
       " ('Alpine', 'mountain'),\n",
       " ('Alpine', 'mountains'),\n",
       " ('Alps', 'mountains'),\n",
       " ('Alps', 'mountain'),\n",
       " ('Alps', 'Swiss'),\n",
       " ('already', 'now'),\n",
       " ('already', 'done'),\n",
       " ('alright', 'okay'),\n",
       " ('alright', 'fine'),\n",
       " ('also', 'too'),\n",
       " ('also', 'and'),\n",
       " ('also', 'as well'),\n",
       " ('altar', 'church'),\n",
       " ('altar', 'wedding'),\n",
       " ('alter', 'change'),\n",
       " ('alteration', 'change'),\n",
       " ('alternate', 'other'),\n",
       " ('alternate', 'different'),\n",
       " ('alternative', 'other'),\n",
       " ('alternative', 'different'),\n",
       " ('alternative', 'music'),\n",
       " ('alternatively', 'other'),\n",
       " ('alternatively', 'otherwise'),\n",
       " ('although', 'but'),\n",
       " ('although', 'however'),\n",
       " ('altitude', 'height'),\n",
       " ('altitude', 'high'),\n",
       " ('alto', 'soprano'),\n",
       " ('alto', 'high'),\n",
       " ('alto', 'sax'),\n",
       " ('alto', 'singer'),\n",
       " ('altruistic', 'giving'),\n",
       " ('aluminum', 'metal'),\n",
       " ('aluminum', 'foil'),\n",
       " ('alumni', 'college'),\n",
       " ('alumni', 'university'),\n",
       " ('alumni', 'school'),\n",
       " ('alumni', 'graduate'),\n",
       " ('Alvin', 'chipmunk'),\n",
       " ('Alvin', 'chipmunks'),\n",
       " ('always', 'forever'),\n",
       " ('always', 'never'),\n",
       " ('am', 'I'),\n",
       " ('am', 'morning'),\n",
       " ('amass', 'gather'),\n",
       " ('amass', 'collect'),\n",
       " ('amateur', 'professional'),\n",
       " ('amaze', 'wonder'),\n",
       " ('amaze', 'wow'),\n",
       " ('amaze', 'surprise'),\n",
       " ('amazement', 'wonder'),\n",
       " ('amazement', 'surprise'),\n",
       " ('amazing', 'wonderful'),\n",
       " ('amazing', 'awesome'),\n",
       " ('Amazon', 'river'),\n",
       " ('ambassador', 'diplomat'),\n",
       " ('amber', 'yellow'),\n",
       " ('ambiance', 'atmosphere'),\n",
       " ('ambiance', 'mood'),\n",
       " ('ambient', 'temperature'),\n",
       " ('ambiguous', 'unclear'),\n",
       " ('ambition', 'drive'),\n",
       " ('amble', 'walk'),\n",
       " ('amble', 'stroll'),\n",
       " ('ambulance', 'emergency'),\n",
       " ('ambulance', 'siren'),\n",
       " ('ambush', 'attack'),\n",
       " ('amen', 'prayer'),\n",
       " ('amen', 'church'),\n",
       " ('amen', 'God'),\n",
       " ('amend', 'fix'),\n",
       " ('amend', 'change'),\n",
       " ('America', 'United States'),\n",
       " ('America', 'USA'),\n",
       " ('amiable', 'friendly'),\n",
       " ('amiable', 'nice'),\n",
       " ('amid', 'among'),\n",
       " ('amid', 'amongst'),\n",
       " ('amino acid', 'protein'),\n",
       " ('amino acid', 'DNA'),\n",
       " ('ammo', 'gun'),\n",
       " ('ammo', 'guns'),\n",
       " ('ammunition', 'gun'),\n",
       " ('ammunition', 'guns'),\n",
       " ('ammunition', 'bullets'),\n",
       " ('amoeba', 'cell'),\n",
       " ('amok', 'run'),\n",
       " ('amok', 'chaos'),\n",
       " ('among', 'with'),\n",
       " ('among', 'within'),\n",
       " ('amongst', 'between'),\n",
       " ('amongst', 'with'),\n",
       " ('amorphous', 'shapeless'),\n",
       " ('amorphous', 'blob'),\n",
       " ('amount', 'money'),\n",
       " ('amount', 'quantity'),\n",
       " ('amp', 'guitar'),\n",
       " ('amphibian', 'frog'),\n",
       " ('ample', 'plenty'),\n",
       " ('amplifier', 'music'),\n",
       " ('amplifier', 'loud'),\n",
       " ('amplifier', 'sound'),\n",
       " ('amplifier', 'speaker'),\n",
       " ('amplify', 'loud'),\n",
       " ('amplify', 'louder'),\n",
       " ('amplify', 'sound'),\n",
       " ('amuse', 'laugh'),\n",
       " ('amuse', 'funny'),\n",
       " ('amuse', 'fun'),\n",
       " ('amusement', 'fun'),\n",
       " ('amusement', 'park'),\n",
       " ('amusing', 'funny'),\n",
       " ('an', 'a'),\n",
       " ('anagram', 'puzzle'),\n",
       " ('anagram', 'words'),\n",
       " ('anal', 'sex'),\n",
       " ('analog', 'digital'),\n",
       " ('analog', 'clock'),\n",
       " ('analogy', 'like'),\n",
       " ('analogy', 'similar'),\n",
       " ('analogy', 'comparison'),\n",
       " ('analysis', 'study'),\n",
       " ('analyze', 'study'),\n",
       " ('analyze', 'think'),\n",
       " ('anarchy', 'chaos'),\n",
       " ('anatomy', 'body'),\n",
       " ('anatomy', 'physiology'),\n",
       " ('ancestor', 'old'),\n",
       " ('ancestors', 'old'),\n",
       " ('ancestry', 'family'),\n",
       " ('ancestry', 'old'),\n",
       " ('anchor', 'ship'),\n",
       " ('anchor', 'boat'),\n",
       " ('anchor', 'weight'),\n",
       " ('anchovy', 'fish'),\n",
       " ('anchovy', 'pizza'),\n",
       " ('ancient', 'old'),\n",
       " ('and', 'or'),\n",
       " ('and', 'also'),\n",
       " ('and', 'plus'),\n",
       " ('and', 'but'),\n",
       " ('android', 'robot'),\n",
       " ('android', 'phone'),\n",
       " ('anecdote', 'story'),\n",
       " ('anemia', 'blood'),\n",
       " ('anew', 'again'),\n",
       " ('anew', 'renew'),\n",
       " ('angel', 'wings'),\n",
       " ('angel', 'heaven'),\n",
       " ('angelic', 'angel'),\n",
       " ('angels', 'heaven'),\n",
       " ('angels', 'wings'),\n",
       " ('anger', 'mad'),\n",
       " ('anger', 'rage'),\n",
       " ('angle', 'right'),\n",
       " ('angle', 'triangle'),\n",
       " ('angles', 'triangles'),\n",
       " ('angles', 'geometry'),\n",
       " ('angry', 'mad'),\n",
       " ('angst', 'worry'),\n",
       " ('anguish', 'pain'),\n",
       " ('animal', 'dog'),\n",
       " ('animate', 'cartoon'),\n",
       " ('animation', 'cartoon'),\n",
       " ('anime', 'cartoon'),\n",
       " ('anime', 'Japan'),\n",
       " ('anime', 'Japanese'),\n",
       " ('anisette', 'licorice'),\n",
       " ('anisette', 'drink'),\n",
       " ('anisette', 'liquor'),\n",
       " ('ankle', 'foot'),\n",
       " ('ankle', 'leg'),\n",
       " ('annihilate', 'destroy'),\n",
       " ('annihilate', 'kill'),\n",
       " ('anniversary', 'wedding'),\n",
       " ('anniversary', 'birthday'),\n",
       " ('announce', 'declare'),\n",
       " ('announcement', 'wedding'),\n",
       " ('annoy', 'bother'),\n",
       " ('annoy', 'irritate'),\n",
       " ('annoyed', 'angry'),\n",
       " ('annoyed', 'irritated'),\n",
       " ('annual', 'yearly'),\n",
       " ('annual', 'year'),\n",
       " ('annually', 'yearly'),\n",
       " ('annually', 'year'),\n",
       " ('anonymous', 'unknown'),\n",
       " ('anorexia', 'thin'),\n",
       " ('anorexia', 'skinny'),\n",
       " ('another', 'one'),\n",
       " ('another', 'more'),\n",
       " ('answer', 'question'),\n",
       " ('answers', 'questions'),\n",
       " ('ant', 'small'),\n",
       " ('ant', 'bug'),\n",
       " ('ant', 'hill'),\n",
       " ('ant', 'insect'),\n",
       " ('Antarctic', 'cold'),\n",
       " ('Antarctic', 'ice'),\n",
       " ('anteater', 'animal'),\n",
       " ('antecedent', 'before'),\n",
       " ('antecedent', 'precedent'),\n",
       " ('antelope', 'deer'),\n",
       " ('antelope', 'animal'),\n",
       " ('antenna', 'TV'),\n",
       " ('antenna', 'television'),\n",
       " ('anterior', 'front'),\n",
       " ('anterior', 'posterior'),\n",
       " ('anthem', 'song'),\n",
       " ('anthem', 'national'),\n",
       " ('anthill', 'ants'),\n",
       " ('anthropology', 'human'),\n",
       " ('anti', 'against'),\n",
       " ('antibiotic', 'medicine'),\n",
       " ('antibiotics', 'medicine'),\n",
       " ('anticipate', 'wait'),\n",
       " ('anticipation', 'excitement'),\n",
       " ('anticipation', 'waiting'),\n",
       " ('antidote', 'poison'),\n",
       " ('antidote', 'cure'),\n",
       " ('antidote', 'medicine'),\n",
       " ('antiquated', 'old'),\n",
       " ('antique', 'old'),\n",
       " ('antiques', 'old'),\n",
       " ('antiseptic', 'clean'),\n",
       " ('antler', 'deer'),\n",
       " ('antlers', 'deer'),\n",
       " ('antlers', 'horns'),\n",
       " ('antlers', 'reindeer'),\n",
       " ('ants', 'insects'),\n",
       " ('ants', 'pants'),\n",
       " ('anus', 'butt'),\n",
       " ('anus', 'bum'),\n",
       " ('anus', 'rectum'),\n",
       " ('anvil', 'hammer'),\n",
       " ('anvil', 'heavy'),\n",
       " ('anxiety', 'stress'),\n",
       " ('anxiety', 'fear'),\n",
       " ('anxious', 'nervous'),\n",
       " ('anxious', 'worried'),\n",
       " ('any', 'all'),\n",
       " ('any', 'some'),\n",
       " ('anybody', 'anyone'),\n",
       " ('anybody', 'everybody'),\n",
       " ('anybody', 'somebody'),\n",
       " ('anybody', 'person'),\n",
       " ('anyone', 'everyone'),\n",
       " ('anyone', 'someone'),\n",
       " ('anything', 'everything'),\n",
       " ('anything', 'nothing'),\n",
       " ('anything', 'something'),\n",
       " ('anytime', 'whenever'),\n",
       " ('anytime', 'now'),\n",
       " ('anytime', 'anywhere'),\n",
       " ('anywhere', 'everywhere'),\n",
       " ('anywhere', 'here'),\n",
       " ('anywhere', 'nowhere'),\n",
       " ('aorta', 'heart'),\n",
       " ('aorta', 'blood'),\n",
       " ('apache', 'Indian'),\n",
       " ('apache', 'helicopter'),\n",
       " ('apart', 'separate'),\n",
       " ('apart', 'together'),\n",
       " ('apartheid', 'South Africa'),\n",
       " ('apartheid', 'Africa'),\n",
       " ('apartheid', 'black'),\n",
       " ('apartment', 'home'),\n",
       " ('apartment', 'flat'),\n",
       " ('apartments', 'flats'),\n",
       " ('apathetic', 'lazy'),\n",
       " ('ape', 'monkey'),\n",
       " ('ape', 'gorilla'),\n",
       " ('apes', 'monkeys'),\n",
       " ('apes', 'monkey'),\n",
       " ('apes', 'gorillas'),\n",
       " ('apex', 'top'),\n",
       " ('apex', 'peak'),\n",
       " ('aphrodisiac', 'sex'),\n",
       " ('aphrodisiac', 'love'),\n",
       " ('apocalypse', 'end'),\n",
       " ('apocalypse', 'now'),\n",
       " ('apologize', 'sorry'),\n",
       " ('apology', 'sorry'),\n",
       " ('apostle', 'Jesus'),\n",
       " ('app', 'phone'),\n",
       " ('app', 'application'),\n",
       " ('apparel', 'clothes'),\n",
       " ('apparel', 'clothing'),\n",
       " ('apparel', 'American'),\n",
       " ('apparent', 'obvious'),\n",
       " ('apparent', 'clear'),\n",
       " ('apparition', 'ghost'),\n",
       " ('appeal', 'sex'),\n",
       " ('appeal', 'court'),\n",
       " ('appealing', 'attractive'),\n",
       " ('appealing', 'nice'),\n",
       " ('appear', 'seem'),\n",
       " ('appear', 'disappear'),\n",
       " ('appear', 'see'),\n",
       " ('appearance', 'looks'),\n",
       " ('appearance', 'look'),\n",
       " ('appendage', 'arm'),\n",
       " ('appendix', 'book'),\n",
       " ('appetite', 'hungry'),\n",
       " ('appetite', 'food'),\n",
       " ('appetite', 'hunger'),\n",
       " ('appetizer', 'food'),\n",
       " ('applaud', 'clap'),\n",
       " ('applause', 'clap'),\n",
       " ('applause', 'clapping'),\n",
       " ('apple', 'fruit'),\n",
       " ('apple', 'orange'),\n",
       " ('apple juice', 'drink'),\n",
       " ('apple pie', 'American'),\n",
       " ('apple pie', 'America'),\n",
       " ('apples', 'oranges'),\n",
       " ('apples', 'fruit'),\n",
       " ('applesauce', 'pork'),\n",
       " ('appliance', 'kitchen'),\n",
       " ('application', 'job'),\n",
       " ('application', 'form'),\n",
       " ('apply', 'job'),\n",
       " ('appointment', 'doctor'),\n",
       " ('appointment', 'date'),\n",
       " ('appointment', 'time'),\n",
       " ('appraise', 'value'),\n",
       " ('appreciate', 'like'),\n",
       " ('appreciate', 'thanks'),\n",
       " ('appreciation', 'gratitude'),\n",
       " ('appreciation', 'thanks'),\n",
       " ('apprehend', 'catch'),\n",
       " ('apprehension', 'fear'),\n",
       " ('apprehensive', 'scared'),\n",
       " ('apprehensive', 'nervous'),\n",
       " ('apprehensive', 'worried'),\n",
       " ('approach', 'near'),\n",
       " ('appropriate', 'right'),\n",
       " ('appropriate', 'inappropriate'),\n",
       " ('approval', 'yes'),\n",
       " ('approve', 'disapprove'),\n",
       " ('approve', 'yes'),\n",
       " ('approve', 'agree'),\n",
       " ('approximate', 'about'),\n",
       " ('approximate', 'close'),\n",
       " ('approximate', 'guess'),\n",
       " ('approximately', 'about'),\n",
       " ('approximately', 'almost'),\n",
       " ('approximation', 'guess'),\n",
       " ('approximation', 'close'),\n",
       " ('approximation', 'about'),\n",
       " ('approximation', 'estimate'),\n",
       " ('apricot', 'fruit'),\n",
       " ('apricot', 'peach'),\n",
       " ('April', 'may'),\n",
       " ('April', 'showers'),\n",
       " ('April', 'month'),\n",
       " ('apron', 'cooking'),\n",
       " ('apron', 'kitchen'),\n",
       " ('apt', 'able'),\n",
       " ('apt', 'appropriate'),\n",
       " ('aptitude', 'ability'),\n",
       " ('aptitude', 'skill'),\n",
       " ('aptitude', 'test'),\n",
       " ('aqua', 'water'),\n",
       " ('aqua', 'blue'),\n",
       " ('aquarium', 'fish'),\n",
       " ('aquarium', 'water'),\n",
       " ('Arab', 'Muslim'),\n",
       " ('Arab', 'Middle East'),\n",
       " ('Arabic', 'language'),\n",
       " ('arachnid', 'spider'),\n",
       " ('arc', 'circle'),\n",
       " ('arcade', 'game'),\n",
       " ('arcade', 'games'),\n",
       " ('archaic', 'old'),\n",
       " ('archaic', 'ancient'),\n",
       " ('archeology', 'dig'),\n",
       " ('archery', 'bow'),\n",
       " ('archery', 'arrow'),\n",
       " ('architect', 'building'),\n",
       " ('architecture', 'building'),\n",
       " ('architecture', 'buildings'),\n",
       " ('archive', 'library'),\n",
       " ('arctic', 'cold'),\n",
       " ('arduous', 'hard'),\n",
       " ('arduous', 'difficult'),\n",
       " ('are', 'is'),\n",
       " ('are', 'you'),\n",
       " ('are', 'be'),\n",
       " ('area', 'space'),\n",
       " ('arena', 'sports'),\n",
       " ('arena', 'stadium'),\n",
       " ('argue', 'fight'),\n",
       " ('argument', 'fight'),\n",
       " ('arid', 'dry'),\n",
       " ('Aries', 'ram'),\n",
       " ('Aries', 'horoscope'),\n",
       " ('Aries', 'zodiac'),\n",
       " ('aright', 'correct'),\n",
       " ('aright', 'okay'),\n",
       " ('arise', 'get up'),\n",
       " ('aristocracy', 'rich'),\n",
       " ('aristocrat', 'rich'),\n",
       " ('aristocratic', 'rich'),\n",
       " ('arithmetic', 'math'),\n",
       " ('arithmetic', 'maths'),\n",
       " ('Arizona', 'state'),\n",
       " ('arm', 'leg'),\n",
       " ('armada', 'Spanish'),\n",
       " ('armada', 'ships'),\n",
       " ('armada', 'fleet'),\n",
       " ('armchair', 'sofa'),\n",
       " ('armed', 'dangerous'),\n",
       " ('armed', 'gun'),\n",
       " ('armed', 'weapon'),\n",
       " ('armed forces', 'army'),\n",
       " ('armed forces', 'military'),\n",
       " ('armor', 'knight'),\n",
       " ('armpit', 'hair'),\n",
       " ('armpit', 'hairy'),\n",
       " ('armpit', 'smelly'),\n",
       " ('arms', 'legs'),\n",
       " ('arms', 'weapons'),\n",
       " ('army', 'navy'),\n",
       " ('army', 'soldier'),\n",
       " ('aroma', 'smell'),\n",
       " ('aroma', 'scent'),\n",
       " ('aromatic', 'smell'),\n",
       " ('aromatic', 'smelly'),\n",
       " ('around', 'about'),\n",
       " ('around', 'circle'),\n",
       " ('arousal', 'sex'),\n",
       " ('arousal', 'sexual'),\n",
       " ('arouse', 'sex'),\n",
       " ('arouse', 'excite'),\n",
       " ('arrange', 'flowers'),\n",
       " ('arrange', 'order'),\n",
       " ('arrangement', 'flowers'),\n",
       " ('arrangement', 'flower'),\n",
       " ('arrest', 'police'),\n",
       " ('arrest', 'jail'),\n",
       " ('arrest', 'stop'),\n",
       " ('arrested', 'police'),\n",
       " ('arrested', 'jail'),\n",
       " ('arrival', 'departure'),\n",
       " ('arrival', 'airport'),\n",
       " ('arrive', 'come'),\n",
       " ('arrive', 'destination'),\n",
       " ('arriving', 'coming'),\n",
       " ('arrow', 'bow'),\n",
       " ('arrow', 'knee'),\n",
       " ('arrows', 'bow'),\n",
       " ('arrows', 'bows'),\n",
       " ('arsenic', 'poison'),\n",
       " ('art', 'painting'),\n",
       " ('artery', 'vein'),\n",
       " ('artery', 'blood'),\n",
       " ('artery', 'heart'),\n",
       " ('arthritis', 'pain'),\n",
       " ('Arthur', 'king'),\n",
       " ('artichoke', 'vegetable'),\n",
       " ('article', 'newspaper'),\n",
       " ('articulate', 'speak'),\n",
       " ('artifact', 'ancient'),\n",
       " ('artifact', 'archeology'),\n",
       " ('artifact', 'old'),\n",
       " ('artificial', 'fake'),\n",
       " ('artillery', 'guns'),\n",
       " ('artillery', 'gun'),\n",
       " ('artisan', 'artist'),\n",
       " ('artisan', 'bread'),\n",
       " ('artist', 'painter'),\n",
       " ('artist', 'paint'),\n",
       " ('artistic', 'creative'),\n",
       " ('arts', 'crafts'),\n",
       " ('artsy', 'artsy'),\n",
       " ('artsy', 'creative'),\n",
       " ('artwork', 'painting'),\n",
       " ('as', 'if'),\n",
       " ('as', 'like'),\n",
       " ('as', 'is'),\n",
       " ('ascend', 'climb'),\n",
       " ('ascend', 'rise'),\n",
       " ('ascend', 'up'),\n",
       " ('ascent', 'climb'),\n",
       " ('ascertain', 'determine'),\n",
       " ('ash', 'fire'),\n",
       " ('ash', 'tree'),\n",
       " ('ashamed', 'embarrassed'),\n",
       " ('ashamed', 'sad'),\n",
       " ('ashes', 'dust'),\n",
       " ('ashes', 'fire'),\n",
       " ('ashes', 'death'),\n",
       " ('ashtray', 'cigarette'),\n",
       " ('ashtray', 'smoke'),\n",
       " ('Asia', 'China'),\n",
       " ('Asia', 'continent'),\n",
       " ('Asian', 'China'),\n",
       " ('Asian', 'Chinese'),\n",
       " ('aside', 'beside'),\n",
       " ('ask', 'question'),\n",
       " ('ask', 'tell'),\n",
       " ('askew', 'crooked'),\n",
       " ('asking', 'question'),\n",
       " ('asleep', 'awake'),\n",
       " ('asleep', 'bed'),\n",
       " ('asparagus', 'green'),\n",
       " ('asparagus', 'vegetable'),\n",
       " ('aspect', 'ratio'),\n",
       " ('aspect', 'view'),\n",
       " ('asphalt', 'road'),\n",
       " ('asphalt', 'pavement'),\n",
       " ('asphalt', 'black'),\n",
       " ('asphyxiate', 'choke'),\n",
       " ('asphyxiate', 'suffocate'),\n",
       " ...]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_association_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free associations Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12217"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplex = nx.Graph()\n",
    "multiplex.add_nodes_from(words)\n",
    "\n",
    "free_assoc = nx.Graph()\n",
    "free_assoc.add_nodes_from(words)\n",
    "len(free_assoc.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the free association layer to the multiplex\n",
    "for pair in free_association_list:\n",
    "    multiplex.add_edge(pair[0], pair[1])\n",
    "    \n",
    "# Add the free association layer to its graph\n",
    "for pair in free_association_list:\n",
    "    free_assoc.add_edge(pair[0], pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12600"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(free_assoc.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mean degree of connectivity _k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9576984126984125"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_degree_connectivity(free_assoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mean Clustering Coefficient _CC_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1269042757133164"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.average_clustering(free_assoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assortativity Coefficient _a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15218634277569376"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.degree_assortativity_coefficient(free_assoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Percentage of nodes in the Largest Connected Component _Conn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_nodes_in_lcc(free_assoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Mean Shortest Path lenght in the Largest Connected Component _d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_shortest_path_lcc(free_assoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have added one layer to multiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18636"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiplex.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r\"free_assoc_full.pickle\", \"rb\") as input_file:\n",
    "#     free_association_list = pickle.load(input_file)\n",
    "    \n",
    "# free_association_list = {a:b for a,b in free_association_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(free_association_list, open( \"free_assoc_full.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the BERT Word Embedding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8691cef7e0d849aca0222e199bd11c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26573 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_emb = {}\n",
    "for word in tqdm(bert_vocab):\n",
    "    sentence = Sentence(word) # --> strip() removes the white space from beginning and end of word\n",
    "     # embed a sentence using glove.\n",
    "    embedding.embed(sentence)\n",
    "    for token in sentence:\n",
    "        word_emb[word]=token.embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_emb['red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_emb['horse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24926"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_emb, open( \"cleaned_word_emb_layer_full.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_layer = nx.Graph()\n",
    "word_emb_layer.add_nodes_from(bert_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance using cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes distance of every word in vocabulary to every other word in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vocab = list(word_emb.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "the\n",
      "of\n",
      "and\n",
      "in\n",
      "to\n",
      "was\n",
      "he\n",
      "is\n",
      "as\n",
      "for\n",
      "on\n",
      "with\n",
      "that\n",
      "it\n",
      "his\n",
      "by\n",
      "at\n",
      "from\n"
     ]
    }
   ],
   "source": [
    "for word in emb_vocab[0:19]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n",
      "finland\n",
      "bare\n",
      "barbara\n",
      "absence\n",
      "ignored\n",
      "dawn\n",
      "injuries\n",
      "producers\n",
      "ram\n",
      "luis\n",
      "ities\n",
      "kw\n",
      "admit\n",
      "expensive\n",
      "electricity\n",
      "nba\n",
      "exception\n",
      "symbol\n"
     ]
    }
   ],
   "source": [
    "for word in emb_vocab[4080:4099]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d95933921b4b3f8ff78a9366afb555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/310652738.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_emb_list = []\n",
    "emb_vocab = list(word_emb.keys())\n",
    "\n",
    "with tqdm(total=len(emb_vocab)*len(emb_vocab)/2.0) as pbar:\n",
    "    while len(emb_vocab) > 0:\n",
    "        item = emb_vocab.pop()\n",
    "        x = word_emb[item]\n",
    "\n",
    "        for word in emb_vocab:\n",
    "            y = word_emb[word]\n",
    "#             print(x.unsqueeze(0))\n",
    "            cosine_sim = torch.cosine_similarity(x.unsqueeze(0), y.unsqueeze(0))\n",
    "            # print(item, word, cosine_sim)\n",
    "            if cosine_sim > word_thresh:\n",
    "                # print(cosine_sim, item, word)\n",
    "                word_emb_list.append((item, word))\n",
    "#                 print(len(word_emb_list))\n",
    "                word_emb_layer.add_edge(item, word)\n",
    "        pbar.update(len(emb_vocab))                   \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By trimming out unused characters and symbols we decrease teh size of our embedding list we iterate over by 84,474,514. Down to 381,321,728 from 465,796,242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('necessitated', 'relies'),\n",
       " ('necessitated', 'supremacy'),\n",
       " ('necessitated', 'footing'),\n",
       " ('necessitated', 'cushion'),\n",
       " ('necessitated', 'cords'),\n",
       " ('necessitated', 'cooks'),\n",
       " ('necessitated', 'continual'),\n",
       " ('necessitated', 'antics'),\n",
       " ('scandals', 'disasters'),\n",
       " ('scandals', 'wonderland')]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_emb_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_emb_list, open( \"cleaned_word_emb_list_95_cos_thresh.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_emb_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mean Degree of connectivity _k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.203081120115542"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_degree_connectivity(word_emb_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was 0.0 in Dr. Kenningtons? / the github repo download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mean Clustering Coefficient _CC_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13787215432622937"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.average_clustering(word_emb_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was 0.0 in Dr. Kennington's / the github repo download :think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assortativity Coefficient _a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.348314097642019"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.degree_assortativity_coefficient(word_emb_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Percentage of nodes in the LCC _Conn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030142192516873074"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_nodes_in_lcc(word_emb_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Mean shortest path in LLC _d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1095732601599093"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_shortest_path_lcc(word_emb_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_emb_layer.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Visual Embedding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('../data/fullmultiplex/clip.bertvocab.embeddings.513.txt', 'r')\n",
    "lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_vecs = {}\n",
    "for line in lines:\n",
    "    visual_vecs[line.split()[0]] = line.split()[1:]\n",
    "    \n",
    "visual_words = list(visual_vecs.keys())\n",
    "\n",
    "len(visual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual_vecs['car'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(visual_vecs, open( \"word_emb_layer_full.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_graph = nx.Graph()\n",
    "visual_graph.add_nodes_from(visual_words)\n",
    "len(visual_graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b6125f607645efa54d789d221a414a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/465796242.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m visual_words_chop:\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#             if word is not item:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m             y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(visual_vecs[word])\n\u001b[0;32m---> 14\u001b[0m             cosine_sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m cosine_sim \u001b[38;5;241m>\u001b[39m visual_thresh:\n\u001b[1;32m     17\u001b[0m                 visual_list\u001b[38;5;241m.\u001b[39mappend((item, word))\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1251\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \n\u001b[1;32m   1219\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03mkernel matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1251\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:156\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    147\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    148\u001b[0m         X,\n\u001b[1;32m    149\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     Y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    165\u001b[0m         Y,\n\u001b[1;32m    166\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:701\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEstimator\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 701\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator_name \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# When all dataframe columns are sparse, convert to a sparse array\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# DataFrame.sparse only supports `to_coo`\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "visual_list = []\n",
    "\n",
    "visual_words_chop = visual_words.copy()\n",
    "\n",
    "with tqdm(total=len(visual_words_chop)*len(visual_words_chop)/2.0) as pbar:\n",
    "    while len(visual_words_chop) > 0:\n",
    "        item = visual_words_chop.pop()\n",
    "        x = np.array(visual_vecs[item])\n",
    "#         print(item, len(visual_words_chop))\n",
    "        for word in visual_words_chop:\n",
    "#             if word is not item:\n",
    "            y = np.array(visual_vecs[word])\n",
    "\n",
    "            cosine_sim = cosine_similarity(x.reshape(1,-1),y.reshape(1,-1))\n",
    "\n",
    "            if cosine_sim > visual_thresh:\n",
    "                visual_list.append((item, word))\n",
    "                visual_graph.add_edge(item, word)\n",
    "        pbar.update(len(visual_words_chop))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual_graph.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mean Degree of Connectivity _k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_degree_connectivity(visual_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mean Clustering Coefficient _CC_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_clustering(visual_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assortativity Coefficient _a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(visual_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Percentage of nodes in the LCC _Conn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_nodes_in_lcc(visual_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Mean shortest length in the LCC _d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shortest_path_lcc(visual_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(visual_list, open( \"visual_layer_vectors_full.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(visual_graph.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Lancaster Embedding Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = pd.read_csv('../data/fullmultiplex/lancaster_full.csv')\n",
    "cols = norms.describe().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all columns\n",
    "for col in cols:\n",
    "    m = norms[col].max()\n",
    "    norms[col] = norms[col] / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = {}\n",
    "\n",
    "for i,row in norms.iterrows():\n",
    "    vecs[row.Word.lower()] =  row[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vecs['red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a cappella',\n",
       " 'aardvark',\n",
       " 'aback',\n",
       " 'abacus',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandonee',\n",
       " 'abandoner',\n",
       " 'abandonment']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_words = list(vecs.keys())\n",
    "\n",
    "lancaster_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vecs, open( \"lancaster_layer_vectors_full.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39707"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = nx.Graph()\n",
    "lancaster.add_nodes_from(lancaster_words)\n",
    "len(lancaster.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f77a8a15f54885b0ba4aaec7688b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/788322924.5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lancaster_list = []\n",
    "\n",
    "lan_words = list(vecs.keys())\n",
    "\n",
    "with tqdm(total=len(lan_words)*len(lan_words)/2.0) as pbar:\n",
    "    while len(lan_words) > 0:\n",
    "        item = lan_words.pop()\n",
    "\n",
    "        x =  vecs[item]\n",
    "    \n",
    "        for word in lan_words:\n",
    "            if word is not item:\n",
    "                y =  vecs[word]\n",
    "                cosine_sim = dot(x, y)/(norm(x)*norm(y))\n",
    "\n",
    "                if cosine_sim > lancaster_thresh:\n",
    "\n",
    "                    lancaster_list.append((item, word))\n",
    "                    lancaster.add_edge(item, word)\n",
    "        pbar.update(len(lan_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39707\n"
     ]
    }
   ],
   "source": [
    "print(len(lancaster.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13114740\n"
     ]
    }
   ],
   "source": [
    "print(len(lancaster_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zucchini', 'bagel'),\n",
       " ('zucchini', 'beef'),\n",
       " ('zucchini', 'bell pepper'),\n",
       " ('zucchini', 'black pudding'),\n",
       " ('zucchini', 'breadstick'),\n",
       " ('zucchini', 'broccoli'),\n",
       " ('zucchini', 'cashew'),\n",
       " ('zucchini', 'cereal'),\n",
       " ('zucchini', 'cheesecake'),\n",
       " ('zucchini', 'cherry')]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lancaster_list, open( \"lancaster_full.pickle\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Mean Degree of Connectivity _k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660.5757171279623"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_degree_connectivity(lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mean Clustering Coefficient _CC_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [276], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlancaster\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/networkx/algorithms/cluster.py:274\u001b[0m, in \u001b[0;36maverage_clustering\u001b[0;34m(G, nodes, weight, count_zeros)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maverage_clustering\u001b[39m(G, nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, count_zeros\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the average clustering coefficient for the graph G.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    The clustering coefficient for the graph is the average,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m       https://arxiv.org/abs/0802.2512\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m count_zeros:\n\u001b[1;32m    276\u001b[0m         c \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m c \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/networkx/algorithms/cluster.py:386\u001b[0m, in \u001b[0;36mclustering\u001b[0;34m(G, nodes, weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         td_iter \u001b[38;5;241m=\u001b[39m _triangles_and_degree_iter(G, nodes)\n\u001b[0;32m--> 386\u001b[0m         clusterc \u001b[38;5;241m=\u001b[39m {v: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t \u001b[38;5;241m/\u001b[39m (d \u001b[38;5;241m*\u001b[39m (d \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m v, d, t, _ \u001b[38;5;129;01min\u001b[39;00m td_iter}\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nodes \u001b[38;5;129;01min\u001b[39;00m G:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Return the value of the sole entry in the dictionary.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clusterc[nodes]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/networkx/algorithms/cluster.py:386\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         td_iter \u001b[38;5;241m=\u001b[39m _triangles_and_degree_iter(G, nodes)\n\u001b[0;32m--> 386\u001b[0m         clusterc \u001b[38;5;241m=\u001b[39m {v: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t \u001b[38;5;241m/\u001b[39m (d \u001b[38;5;241m*\u001b[39m (d \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m v, d, t, _ \u001b[38;5;129;01min\u001b[39;00m td_iter}\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nodes \u001b[38;5;129;01min\u001b[39;00m G:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Return the value of the sole entry in the dictionary.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clusterc[nodes]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/networkx/algorithms/cluster.py:77\u001b[0m, in \u001b[0;36m_triangles_and_degree_iter\u001b[0;34m(G, nodes)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, v_nbrs \u001b[38;5;129;01min\u001b[39;00m nodes_nbrs:\n\u001b[1;32m     76\u001b[0m     vs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(v_nbrs) \u001b[38;5;241m-\u001b[39m {v}\n\u001b[0;32m---> 77\u001b[0m     gen_degree \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mw\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     ntriangles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(k \u001b[38;5;241m*\u001b[39m val \u001b[38;5;28;01mfor\u001b[39;00m k, val \u001b[38;5;129;01min\u001b[39;00m gen_degree\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (v, \u001b[38;5;28mlen\u001b[39m(vs), ntriangles, gen_degree)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/collections/__init__.py:552\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/cs596-final-proj-M-BMq5HJ-py3.8/lib/python3.8/site-packages/networkx/algorithms/cluster.py:77\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v, v_nbrs \u001b[38;5;129;01min\u001b[39;00m nodes_nbrs:\n\u001b[1;32m     76\u001b[0m     vs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(v_nbrs) \u001b[38;5;241m-\u001b[39m {v}\n\u001b[0;32m---> 77\u001b[0m     gen_degree \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mlen\u001b[39m(vs \u001b[38;5;241m&\u001b[39m (\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m {w})) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m vs)\n\u001b[1;32m     78\u001b[0m     ntriangles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(k \u001b[38;5;241m*\u001b[39m val \u001b[38;5;28;01mfor\u001b[39;00m k, val \u001b[38;5;129;01min\u001b[39;00m gen_degree\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (v, \u001b[38;5;28mlen\u001b[39m(vs), ntriangles, gen_degree)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nx.average_clustering(lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assortativity Coefficient _a_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.degree_assortativity_coefficient(lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Percetage of nodes in the LCC _Conn._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_nodes_in_lcc(lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Mean shortest path length in LCC _d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shortest_path_lcc(lancaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lancaster.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we will add the above created layers to the multiplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"free_assoc_full.pickle\", \"rb\") as input_file:\n",
    "    free_assoc = pickle.load(input_file)\n",
    "    \n",
    "with open(r\"word_emb_layer_full.pickle\", \"rb\") as input_file:\n",
    "    word_emb_list = pickle.load(input_file)    \n",
    "    \n",
    "with open(r\"visual_graph_full.pickle\", \"rb\") as input_file:\n",
    "    visual_list = pickle.load(input_file)        \n",
    "    \n",
    "with open(r\"lancaster_full.pickle\", \"rb\") as input_file:\n",
    "    lancaster_list = pickle.load(input_file)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(free_assoc))\n",
    "print(len(word_emb_list))\n",
    "print(len(visual_list))\n",
    "print(len(lancaster_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = {'free_assoc':free_assoc,\n",
    "          'word_emb_list':word_emb_list,\n",
    "          'visual_list':visual_list, \n",
    "          'lancaster_list':lancaster_list}\n",
    "\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return list(chain.from_iterable(combinations(s, r) for r in range(len(s)+1)))\n",
    "\n",
    "all_subsets = powerset(layers.keys())\n",
    "all_subsets.pop(0) # the first subset is the empty set\n",
    "print(len(all_subsets))\n",
    "all_subsets[14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_layers in all_subsets[14:]:\n",
    "    print(sub_layers)\n",
    "    \n",
    "    words = []\n",
    "    for k in sub_layers:\n",
    "        i = layers[k]\n",
    "        for a,b in i:\n",
    "            words.append(a)\n",
    "            words.append(b)\n",
    "\n",
    "    words = list(set(words)) \n",
    "    print(len(words))\n",
    "\n",
    "    new_3 = nx.Graph()\n",
    "#     new_3.add_nodes_from(words)\n",
    "    \n",
    "    for k in sub_layers:\n",
    "        for pair in layers[k]:\n",
    "            new_3.add_edge(pair[0], pair[1])    \n",
    "\n",
    "    print(len(new_3.nodes))\n",
    "    print('k', mean_degree_connectivity(new_3))\n",
    "    print('CC', nx.average_clustering(new_3))\n",
    "    print('a', nx.degree_assortativity_coefficient(new_3))\n",
    "    print('conn', perc_nodes_in_lcc(new_3))\n",
    "    print('d', mean_shortest_path_lcc(new_3))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('free_assoc',)\n",
    "13744\n",
    "k 5.968277066356229\n",
    "CC 0.18140117405816272\n",
    "a -0.1378838837754419\n",
    "conn 0.9985448195576252\n",
    "d 4.9834091852721665\n",
    "\n",
    "('word_emb_list',)\n",
    "25558\n",
    "k 1119.9501525940998\n",
    "CC 0.6476716252456551\n",
    "a 0.1769889275645062\n",
    "conn 0.8268643868847327\n",
    "d 3.069093343628048\n",
    "\n",
    "('visual_list',)\n",
    "11811\n",
    "k 2464.240284480569\n",
    "CC 0.8647897461330403\n",
    "a -0.4776667293884356\n",
    "conn 0.9993226653119973\n",
    "d 1.868065777870662\n",
    "\n",
    "('lancaster_list',)\n",
    "39662\n",
    "k 2542.666784327568\n",
    "CC 0.5281151568371529"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
