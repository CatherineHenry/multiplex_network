{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Child vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab = [line.strip() for line in open('../data/vertomul.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_assoc = pickle.load( open( \"free_assoc.pickle\", \"rb\" ))\n",
    "co_oc = pickle.load( open( \"co_oc.pickle\", \"rb\" ))\n",
    "phon_conn = pickle.load( open( \"phon_conn.pickle\", \"rb\" ))\n",
    "feat_norms = pickle.load( open( \"feat_norms.pickle\", \"rb\" ))\n",
    "word_emb_layer = pickle.load( open( \"word_emb_layer.pickle\", \"rb\" ))\n",
    "visual_layer = pickle.load( open( \"visual_graph.pickle\", \"rb\" ))\n",
    "lancaster_layer = pickle.load( open( \"lancaster.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection between two sets -- what they have in common\n",
    "len(set(free_assoc) & set(lancaster_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancaster_layer - it introduces new nodes \n",
    "lists = [ co_oc, phon_conn, free_assoc, feat_norms, word_emb_layer, visual_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct weighted graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate.add_nodes_from(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529\n",
      "529\n",
      "529\n",
      "529\n",
      "529\n",
      "529\n"
     ]
    }
   ],
   "source": [
    "# setting the weights. If the connection between two nodes is established across multiple layers, we up its weight\n",
    "for a_list in lists:\n",
    " \n",
    "    for pair in a_list:\n",
    "      \n",
    "        if aggregate.has_edge(pair[0], pair[1]):\n",
    "            aggregate[pair[0]][pair[1]]['weight'] += 1\n",
    "        else:\n",
    "#             print(\"adding nodes on\", pair[0], pair[1])\n",
    "            aggregate.add_edge(pair[0], pair[1], weight=1)\n",
    "    print(aggregate.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ISSUE HERE!! The network has more nodes than the vocab. Caused by the Lancaster layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(G):\n",
    "    '''\n",
    "     This function creates a list of\n",
    "     normalized weights for each node \n",
    "     in the G graph. The sum of the \n",
    "     weights sum to 1.\n",
    "    '''\n",
    "    normalized_weights = []\n",
    "    weights = list(G.degree(weight='weight')) # strength of each node\n",
    "    overall = sum(node[1] for node in weights) # sum of all weights in graph \n",
    "    \n",
    "    for node in weights:\n",
    "        perc = node[1]/overall\n",
    "        normalized_weights.append(perc)\n",
    "    \n",
    "    return normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_connections(G, batch):\n",
    "    '''\n",
    "     This function returns a list of\n",
    "     tuples with (node, edge) pairings\n",
    "     for each connection a node has. \n",
    "    '''\n",
    "    list_conns = []\n",
    "    for node in batch:\n",
    "        list_conns.append(G.edges(node))\n",
    "    return list_conns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    " def preferential_attachment(G, vocab, weights, batch_size):\n",
    "    '''\n",
    "     preferential attachment function call takes in a graph \n",
    "     and a list of tuples --> for each node in the batch list,\n",
    "     get all of its edges and create a list of tuples.\n",
    "    '''\n",
    "    \n",
    "    # create a random batch of nodes\n",
    "    batch = np.random.choice(vocab, batch_size, p=weights)\n",
    "    \n",
    "    # create a list of tuples (node, edge) for each edge a node has\n",
    "    list_conns = get_node_connections(G, batch)\n",
    "    \n",
    "    node_avgs_list = []\n",
    "    for arr in list_conns:\n",
    "\n",
    "        preds = nx.preferential_attachment(aggregate, arr)\n",
    "        node_avgs = np.mean([p for u,v,p in preds])\n",
    "        node_avgs_list.append(node_avgs)\n",
    "    \n",
    "    zip_lists = list(zip(batch, node_avgs_list))\n",
    "    \n",
    "    print(zip_lists) # list with averages preferential attachment scorer for each node in batch\n",
    "    \n",
    "    return batch_avg, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('daddy', 1864.0), ('street', 554.0), ('now', 2864.0), ('cold', 1497.0), ('walk', 1152.0), ('bus', 2985.0), ('hair', 1505.0), ('time', 1781.0), ('sing', 991.0), ('tiger', 3344.0), ('nice', 1647.0), ('dress', 3162.0), ('horse', 4015.0), ('couch', 3190.0), ('truck', 2794.0), ('potty', 1417.0), ('little', 2475.0), ('penguin', 3331.0), ('buy', 1400.0), ('kiss', 412.0)]\n",
      "\n",
      "batch average:  2119.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2119.0,\n",
       " array(['daddy', 'street', 'now', 'cold', 'walk', 'bus', 'hair', 'time',\n",
       "        'sing', 'tiger', 'nice', 'dress', 'horse', 'couch', 'truck',\n",
       "        'potty', 'little', 'penguin', 'buy', 'kiss'], dtype='<U12'))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferential_attachment(aggregate, vocab, normalized_weights, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('toothbrush', 46.0), ('broken', 105.0), ('farm', 742.0), ('ouch', 171.0), ('jello', 147.0), ('full', 435.0), ('pour', 520.0), ('hand', 1524.0), ('sink', 1716.0), ('sidewalk', 75.0), ('why', 2781.0), ('long', 1317.0), ('banana', 3554.0), ('tractor', 3526.0), ('face', 857.0), ('snow', 548.0), ('now', 2864.0), ('build', 1039.0), ('tooth', 243.0), ('high', 409.0)]\n",
      "\n",
      "batch average:  1130.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1130.95,\n",
       " array(['toothbrush', 'broken', 'farm', 'ouch', 'jello', 'full', 'pour',\n",
       "        'hand', 'sink', 'sidewalk', 'why', 'long', 'banana', 'tractor',\n",
       "        'face', 'snow', 'now', 'build', 'tooth', 'high'], dtype='<U12'))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferential_attachment(aggregate, vocab, None, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
